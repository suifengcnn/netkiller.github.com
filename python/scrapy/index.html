<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>第 11 章 Scrapy - Python web scraping and crawling framework</title><link rel="stylesheet" type="text/css" href="..//docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><meta name="keywords" content="php, pear, pecl, phar" /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="../index.html" title="Netkiller Python 手札" /><link rel="prev" href="../library/zope.testbrowser.html" title="10.18. zope.testbrowser" /><link rel="next" href="scrapy.html" title="11.2. scrapy 命令" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> |
		<a xmlns="" href="//netkiller.github.io/">简体中文</a> |
	    <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> |
	    <a xmlns="" href="/journal/index.html">杂文</a> |
	    <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> |
	    <a xmlns="" href="http://netkiller-github-com.iteye.com/">ITEYE 博客</a> |
	    <a xmlns="" href="http://my.oschina.net/neochen/">OSChina 博客</a> |
	    <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> |
	    <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> |
	    <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> |
	    <a xmlns="" href="/search.html">Search</a> |
		<a xmlns="" href="mailto:netkiller@msn.com">Email</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">第 11 章 Scrapy - Python web scraping and crawling framework</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="scrapy.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td></tr></table><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="index"></a>第 11 章 Scrapy - Python web scraping and crawling framework</h1></div></div></div><div class="toc"><p><strong>目录</strong></p><dl class="toc"><dt><span class="section"><a href="index.html#setup">11.1. 安装 scrapy 开发环境</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idp48">11.1.1. Mac</a></span></dt><dt><span class="section"><a href="index.html#idp49">11.1.2. Ubuntu</a></span></dt><dt><span class="section"><a href="index.html#idp50">11.1.3. 使用 pip 安装 scrapy</a></span></dt><dt><span class="section"><a href="index.html#idp51">11.1.4. 测试 scrapy</a></span></dt></dl></dd><dt><span class="section"><a href="scrapy.html">11.2. scrapy 命令</a></span></dt><dd><dl><dt><span class="section"><a href="scrapy.html#scrapy.startproject">11.2.1. </a></span></dt><dt><span class="section"><a href="scrapy.html#scrapy.genspider">11.2.2. 新建 spider</a></span></dt><dt><span class="section"><a href="scrapy.html#scrapy.list">11.2.3. 列出可用的 spiders</a></span></dt><dt><span class="section"><a href="scrapy.html#scrapy.crawl">11.2.4. 运行 spider</a></span></dt></dl></dd><dt><span class="section"><a href="shell.html">11.3. Scrapy Shell</a></span></dt><dd><dl><dt><span class="section"><a href="shell.html#response">11.3.1. response</a></span></dt><dd><dl><dt><span class="section"><a href="shell.html#response.url">11.3.1.1. 当前URL地址</a></span></dt><dt><span class="section"><a href="shell.html#status">11.3.1.2. status HTTP 状态</a></span></dt><dt><span class="section"><a href="shell.html#text">11.3.1.3. text 正文</a></span></dt><dt><span class="section"><a href="shell.html#response.css">11.3.1.4. css</a></span></dt><dd><dl><dt><span class="section"><a href="shell.html#idp52">11.3.1.4.1. 获取 html 属性</a></span></dt></dl></dd><dt><span class="section"><a href="shell.html#response.xpath">11.3.1.5. xpath</a></span></dt><dt><span class="section"><a href="shell.html#headers">11.3.1.6. headers</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="tutorial.html">11.4. 爬虫项目</a></span></dt><dd><dl><dt><span class="section"><a href="tutorial.html#startproject">11.4.1. 创建项目</a></span></dt><dt><span class="section"><a href="tutorial.html#spider">11.4.2. Spider</a></span></dt><dd><dl><dt><span class="section"><a href="tutorial.html#response.follow">11.4.2.1. 翻页操作</a></span></dt><dt><span class="section"><a href="tutorial.html#response.body">11.4.2.2. 采集内容保存到文件</a></span></dt></dl></dd><dt><span class="section"><a href="tutorial.html#settings">11.4.3. settings.py 爬虫配置文件</a></span></dt><dd><dl><dt><span class="section"><a href="tutorial.html#idp53">11.4.3.1. 忽略 robots.txt 规则</a></span></dt></dl></dd><dt><span class="section"><a href="tutorial.html#items">11.4.4. Item</a></span></dt><dt><span class="section"><a href="tutorial.html#pipeline">11.4.5. Pipeline</a></span></dt></dl></dd><dt><span class="section"><a href="images.html">11.5. 下载图片</a></span></dt><dd><dl><dt><span class="section"><a href="images.html#idp54">11.5.1. 配置 settings.py</a></span></dt><dt><span class="section"><a href="images.html#idp55">11.5.2. 修改 pipelines.py 文件</a></span></dt><dt><span class="section"><a href="images.html#idp56">11.5.3. 编辑 items.py</a></span></dt><dt><span class="section"><a href="images.html#idp57">11.5.4. Spider 爬虫文件</a></span></dt></dl></dd><dt><span class="section"><a href="xpath.html">11.6. xpath</a></span></dt><dd><dl><dt><span class="section"><a href="xpath.html#idp60">11.6.1. 逻辑运算符</a></span></dt><dd><dl><dt><span class="section"><a href="xpath.html#idp58">11.6.1.1. and</a></span></dt><dt><span class="section"><a href="xpath.html#idp59">11.6.1.2. or</a></span></dt></dl></dd><dt><span class="section"><a href="xpath.html#fun">11.6.2. function</a></span></dt><dd><dl><dt><span class="section"><a href="xpath.html#text()">11.6.2.1. text()</a></span></dt><dt><span class="section"><a href="xpath.html#contains()">11.6.2.2. contains()</a></span></dt></dl></dd></dl></dd></dl></div>
	
	<p>https://scrapy.org</p>
	<div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="setup"></a>11.1. 安装 scrapy 开发环境</h2></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idp48"></a>11.1.1. Mac</h3></div></div></div>
			
			<pre class="screen">
neo@MacBook-Pro ~ % brew install python3
neo@MacBook-Pro ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idp49"></a>11.1.2. Ubuntu</h3></div></div></div>
			
			<p>搜索 scrapy 包，scrapy 支持 Python2.7 和 Python3 我们只需要 python3 版本</p>
			<pre class="screen">
neo@netkiller ~ % apt-cache search scrapy | grep python3
python3-scrapy - Python web scraping and crawling framework (Python 3)
python3-scrapy-djangoitem - Scrapy extension to write scraped items using Django models (Python3 version)
python3-w3lib - Collection of web-related functions (Python 3)			
			</pre>
			<p>Ubuntu 17.04 默认 scrapy 版本为 1.3.0-1 如果需要最新的 1.4.0 请使用 pip 命令安装</p>
			<pre class="screen">
neo@netkiller ~ % apt search python3-scrapy
Sorting... Done
Full Text Search... Done
python3-scrapy/zesty,zesty 1.3.0-1~exp2 all
  Python web scraping and crawling framework (Python 3)

python3-scrapy-djangoitem/zesty,zesty 1.1.1-1 all
  Scrapy extension to write scraped items using Django models (Python3 version)
			</pre>
			<p>安装 scrapy</p>
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-scrapy
[sudo] password for neo: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib python3-wcwidth
  python3-webencodings python3-zope.interface
Suggested packages:
  python-pexpect-doc python-attr-doc python-cryptography-doc python3-cryptography-vectors python3-genshi python3-lxml-dbg python-lxml-doc default-mysql-server | virtual-mysql-server
  python-egenix-mxdatetime python3-mysqldb-dbg python-openssl-doc python3-openssl-dbg python3-pam-dbg python-pil-doc python3-pil-dbg doc-base python-pydispatch-doc ttf-bitstream-vera python-scrapy-doc
  python3-wxgtk3.0 | python3-wxgtk python-setuptools-doc python3-tk python3-gtk2 python3-glade2 python3-qt4 python3-wxgtk2.8 python3-twisted-bin-dbg
The following NEW packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-scrapy python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib
  python3-wcwidth python3-webencodings python3-zope.interface
0 upgraded, 49 newly installed, 0 to remove and 0 not upgraded.
Need to get 7,152 kB of archives.
After this operation, 40.8 MB of additional disk space will be used.
Do you want to continue? [Y/n]
			</pre>
			<p>输入大写 “Y” 然后回车</p>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idp50"></a>11.1.3. 使用 pip 安装 scrapy</h3></div></div></div>
			
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-pip
neo@netkiller ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idp51"></a>11.1.4. 测试 scrapy</h3></div></div></div>
			
			<p>创建测试程序，用于验证 scrapy 安装是否存在问题。</p>
			<pre class="screen">
			
$ cat &gt; myspider.py &lt;&lt;EOF
import scrapy

class BlogSpider(scrapy.Spider):
    name = 'blogspider'
    start_urls = ['https://blog.scrapinghub.com']

    def parse(self, response):
        for title in response.css('h2.entry-title'):
            yield {'title': title.css('a ::text').extract_first()}

        for next_page in response.css('div.prev-post &gt; a'):
            yield response.follow(next_page, self.parse)
EOF
			
			</pre>
			<p>运行爬虫</p>
			<pre class="screen">
$ scrapy runspider myspider.py
			</pre>
		</div>
	</div>
	
	
	
	
	
</div><div xmlns="" id="disqus_thread"></div><script xmlns="">

var disqus_config = function () {
this.page.url = "http://www.netkiller.cn";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'netkiller'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//netkiller.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript xmlns="">Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><br xmlns="" /><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="scrapy.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">10.18. zope.testbrowser </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 11.2. scrapy 命令</td></tr></table></div><script xmlns="">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11694057-1', 'auto');
  ga('send', 'pageview');

</script><script xmlns="" async="async">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script xmlns="" async="async">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><script xmlns="" type="text/javascript" src="/js/q.js" async="async"></script></body></html>