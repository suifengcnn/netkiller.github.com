<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>第 11 章 Crawler</title><link rel="stylesheet" type="text/css" href="..//docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><meta name="keywords" content="php, pear, pecl, phar" /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="../index.html" title="Netkiller Python 手札" /><link rel="prev" href="../library/zope.testbrowser.html" title="10.18. zope.testbrowser" /><link rel="next" href="../3rdparty/index.html" title="第 12 章 3rdparty toolkit" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> |
		<a xmlns="" href="//netkiller.github.io/">简体中文</a> |
	    <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> |
	    <a xmlns="" href="/journal/index.html">杂文</a> |
	    <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> |
	    <a xmlns="" href="http://netkiller-github-com.iteye.com/">ITEYE 博客</a> |
	    <a xmlns="" href="http://my.oschina.net/neochen/">OSChina 博客</a> |
	    <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> |
	    <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> |
	    <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> |
	    <a xmlns="" href="/search.html">Search</a> |
		<a xmlns="" href="mailto:netkiller@msn.com">Email</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">第 11 章 Crawler</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="../3rdparty/index.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td></tr></table><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="index"></a>第 11 章 Crawler</h1></div></div></div><div class="toc"><p><strong>目录</strong></p><dl class="toc"><dt><span class="section"><a href="index.html#scrapy">11.1. scrapy</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#setup">11.1.1. 安装 scrapy 开发环境</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idp48">11.1.1.1. Mac</a></span></dt><dt><span class="section"><a href="index.html#idp49">11.1.1.2. Ubuntu</a></span></dt><dt><span class="section"><a href="index.html#idp50">11.1.1.3. 使用 pip 安装 scrapy</a></span></dt><dt><span class="section"><a href="index.html#idp51">11.1.1.4. 测试 scrapy</a></span></dt></dl></dd><dt><span class="section"><a href="index.html#shell">11.1.2. Scrapy Shell</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#response">11.1.2.1. response</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#status">11.1.2.1.1. status HTTP 状态</a></span></dt><dt><span class="section"><a href="index.html#text">11.1.2.1.2. text 正文</a></span></dt><dt><span class="section"><a href="index.html#response.css">11.1.2.1.3. css</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idp52">11.1.2.1.3.1. 获取 html 属性</a></span></dt></dl></dd><dt><span class="section"><a href="index.html#response.xpath">11.1.2.1.4. xpath</a></span></dt><dt><span class="section"><a href="index.html#headers">11.1.2.1.5. headers</a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="index.html#tutorial">11.1.3. 爬虫项目</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#startproject">11.1.3.1. 创建项目</a></span></dt><dt><span class="section"><a href="index.html#spider">11.1.3.2. Spider</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#response.follow">11.1.3.2.1. 翻页操作</a></span></dt></dl></dd></dl></dd></dl></dd></dl></div>
	
<div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="scrapy"></a>11.1. scrapy</h2></div></div></div>
	
	<p>https://scrapy.org</p>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="setup"></a>11.1.1. 安装 scrapy 开发环境</h3></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp48"></a>11.1.1.1. Mac</h4></div></div></div>
			
			<pre class="screen">
neo@MacBook-Pro ~ % brew install python3
neo@MacBook-Pro ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp49"></a>11.1.1.2. Ubuntu</h4></div></div></div>
			
			<p>搜索 scrapy 包，scrapy 支持 Python2.7 和 Python3 我们只需要 python3 版本</p>
			<pre class="screen">
neo@netkiller ~ % apt-cache search scrapy | grep python3
python3-scrapy - Python web scraping and crawling framework (Python 3)
python3-scrapy-djangoitem - Scrapy extension to write scraped items using Django models (Python3 version)
python3-w3lib - Collection of web-related functions (Python 3)			
			</pre>
			<p>Ubuntu 17.04 默认 scrapy 版本为 1.3.0-1 如果需要最新的 1.4.0 请使用 pip 命令安装</p>
			<pre class="screen">
neo@netkiller ~ % apt search python3-scrapy
Sorting... Done
Full Text Search... Done
python3-scrapy/zesty,zesty 1.3.0-1~exp2 all
  Python web scraping and crawling framework (Python 3)

python3-scrapy-djangoitem/zesty,zesty 1.1.1-1 all
  Scrapy extension to write scraped items using Django models (Python3 version)
			</pre>
			<p>安装 scrapy</p>
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-scrapy
[sudo] password for neo: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib python3-wcwidth
  python3-webencodings python3-zope.interface
Suggested packages:
  python-pexpect-doc python-attr-doc python-cryptography-doc python3-cryptography-vectors python3-genshi python3-lxml-dbg python-lxml-doc default-mysql-server | virtual-mysql-server
  python-egenix-mxdatetime python3-mysqldb-dbg python-openssl-doc python3-openssl-dbg python3-pam-dbg python-pil-doc python3-pil-dbg doc-base python-pydispatch-doc ttf-bitstream-vera python-scrapy-doc
  python3-wxgtk3.0 | python3-wxgtk python-setuptools-doc python3-tk python3-gtk2 python3-glade2 python3-qt4 python3-wxgtk2.8 python3-twisted-bin-dbg
The following NEW packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-scrapy python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib
  python3-wcwidth python3-webencodings python3-zope.interface
0 upgraded, 49 newly installed, 0 to remove and 0 not upgraded.
Need to get 7,152 kB of archives.
After this operation, 40.8 MB of additional disk space will be used.
Do you want to continue? [Y/n]
			</pre>
			<p>输入大写 “Y” 然后回车</p>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp50"></a>11.1.1.3. 使用 pip 安装 scrapy</h4></div></div></div>
			
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-pip
neo@netkiller ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp51"></a>11.1.1.4. 测试 scrapy</h4></div></div></div>
			
			<p>创建测试程序，用于验证 scrapy 安装是否存在问题。</p>
			<pre class="screen">
			
$ cat &gt; myspider.py &lt;&lt;EOF
import scrapy

class BlogSpider(scrapy.Spider):
    name = 'blogspider'
    start_urls = ['https://blog.scrapinghub.com']

    def parse(self, response):
        for title in response.css('h2.entry-title'):
            yield {'title': title.css('a ::text').extract_first()}

        for next_page in response.css('div.prev-post &gt; a'):
            yield response.follow(next_page, self.parse)
EOF
			
			</pre>
			<p>运行爬虫</p>
			<pre class="screen">
$ scrapy runspider myspider.py
			</pre>
		</div>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="shell"></a>11.1.2. Scrapy Shell</h3></div></div></div>
		
		<p>Scrapy Shell 是一个爬虫命令行交互界面调试工具，可以使用它分析被爬的页面</p>
		<pre class="screen">
		
neo@MacBook-Pro /tmp % scrapy shell http://www.netkiller.cn
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-09-01 15:23:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-09-01 15:23:05 [scrapy.core.engine] INFO: Spider opened
2017-09-01 15:23:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn&gt; (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x103b2afd0&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.netkiller.cn&gt;
[s]   response   &lt;200 http://www.netkiller.cn&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x1049019e8&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x104be2a90&gt;
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects 
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
&gt;&gt;&gt; 
		
		</pre>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="response"></a>11.1.2.1. response</h4></div></div></div>
			
			<p>response 是爬虫返回的页面，可以通过 css(), xpath() 等方法取出你需要的内容。</p><div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="status"></a>11.1.2.1.1. status HTTP 状态</h5></div></div></div>
				
				<pre class="screen">
&gt;&gt;&gt; response.status
200
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="text"></a>11.1.2.1.2. text 正文</h5></div></div></div>
				
				<p>返回 HTML 页面正文</p>
				<pre class="screen">
response.text
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.css"></a>11.1.2.1.3. css</h5></div></div></div>
				
				<p>css() 这个方法可以用来选择html和css</p>
				<pre class="screen">
				
&gt;&gt;&gt; response.css('title')
[&lt;Selector xpath='descendant-or-self::title' data='&lt;title&gt;Netkiller ebook - Linux ebook&lt;/ti'&gt;]

&gt;&gt;&gt; response.css('title').extract()
['&lt;title&gt;Netkiller ebook - Linux ebook&lt;/title&gt;']

&gt;&gt;&gt; response.css('title::text').extract()
['Netkiller ebook - Linux ebook']
				
				</pre>
				<p>基于 class 选择</p>
				<pre class="screen">
				
&gt;&gt;&gt; response.css('a.ulink')[1].extract()
'&lt;a class="ulink" href="http://netkiller.github.io/" target="_top"&gt;http://netkiller.github.io&lt;/a&gt;'	

&gt;&gt;&gt; response.css('a.ulink::text')[3].extract()
'http://netkiller.sourceforge.net'
				
				</pre>
				<p>数组的处理</p>
				<pre class="screen">
				
&gt;&gt;&gt; response.css('a::text').extract_first()
'简体中文'

&gt;&gt;&gt; response.css('a::text')[1].extract()
'繁体中文'

&gt;&gt;&gt; response.css('div.blockquote')[1].css('a.ulink::text').extract()
['Netkiller Architect 手札', 'Netkiller Developer 手札', 'Netkiller PHP 手札', 'Netkiller Python 手札', 'Netkiller Testing 手札', 'Netkiller Java 手札', 'Netkiller Cryptography 手札', 'Netkiller Linux 手札', 'Netkiller FreeBSD 手札', 'Netkiller Shell 手札', 'Netkiller Security 手札', 'Netkiller Web 手札', 'Netkiller Monitoring 手札', 'Netkiller Storage 手札', 'Netkiller Mail 手札', 'Netkiller Docbook 手札', 'Netkiller Project 手札', 'Netkiller Database 手札', 'Netkiller PostgreSQL 手札', 'Netkiller MySQL 手札', 'Netkiller NoSQL 手札', 'Netkiller LDAP 手札', 'Netkiller Network 手札', 'Netkiller Cisco IOS 手札', 'Netkiller H3C 手札', 'Netkiller Multimedia 手札', 'Netkiller Perl 手札', 'Netkiller Amateur Radio 手札']
				
				</pre>
				<p>正则表达式</p>
				<pre class="screen">
				
&gt;&gt;&gt; response.css('title::text').re(r'Netkiller.*')
['Netkiller ebook - Linux ebook']

&gt;&gt;&gt; response.css('title::text').re(r'N\w+')
['Netkiller']

&gt;&gt;&gt; response.css('title::text').re(r'(\w+) (\w+)')
['Netkiller', 'ebook', 'Linux', 'ebook']
				
				</pre>
				<div class="section"><div class="titlepage"><div><div><h6 class="title"><a id="idp52"></a>11.1.2.1.3.1. 获取 html 属性</h6></div></div></div>
					
					<p>通过 a::attr() 可以获取 html 标记的属性值</p>
					<pre class="screen">
					
&gt;&gt;&gt; response.css('td a::attr(href)').extract_first()
'http://netkiller.github.io/'					
					
					</pre>
				</div>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.xpath"></a>11.1.2.1.4. xpath</h5></div></div></div>
				
				<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//title')
[&lt;Selector xpath='//title' data='&lt;title&gt;Netkiller ebook - Linux ebook&lt;/ti'&gt;]

&gt;&gt;&gt; response.xpath('//title/text()').extract_first()
'Netkiller ebook - Linux ebook'				
				
				</pre>
				<p>xpath 也可以使用 re() 方法做正则处理</p>
				<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//title/text()').re(r'(\w+)')
['Netkiller', 'ebook', 'Linux', 'ebook']				
				
				</pre>
				<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//img/@src').extract()
['graphics/spacer.gif', 'graphics/note.gif', 'graphics/by-nc-sa.png', '/images/weixin.jpg', 'images/neo.jpg', '/images/weixin.jpg']
				
				</pre>
				<pre class="screen">
				
&gt;&gt;&gt; response.xpath('//a/@href')[0].extract()
'http://netkiller.github.io/'

&gt;&gt;&gt; response.xpath('//a/text()')[0].extract()
'简体中文'

&gt;&gt;&gt; response.xpath('//div[@class="blockquote"]')[1].css('a.ulink::text').extract()
['Netkiller Architect 手札', 'Netkiller Developer 手札', 'Netkiller PHP 手札', 'Netkiller Python 手札', 'Netkiller Testing 手札', 'Netkiller Java 手札', 'Netkiller Cryptography 手札', 'Netkiller Linux 手札', 'Netkiller FreeBSD 手札', 'Netkiller Shell 手札', 'Netkiller Security 手札', 'Netkiller Web 手札', 'Netkiller Monitoring 手札', 'Netkiller Storage 手札', 'Netkiller Mail 手札', 'Netkiller Docbook 手札', 'Netkiller Project 手札', 'Netkiller Database 手札', 'Netkiller PostgreSQL 手札', 'Netkiller MySQL 手札', 'Netkiller NoSQL 手札', 'Netkiller LDAP 手札', 'Netkiller Network 手札', 'Netkiller Cisco IOS 手札', 'Netkiller H3C 手札', 'Netkiller Multimedia 手札', 'Netkiller Perl 手札', 'Netkiller Amateur Radio 手札']

				
				</pre>
				<pre class="screen">
				
				</pre>
				<pre class="screen">
				
				</pre>
			</div>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="headers"></a>11.1.2.1.5. headers</h5></div></div></div>
				
				<pre class="screen">
response.headers.getlist('Set-Cookie')
				</pre>
			</div>
			
			

			
		</div>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="tutorial"></a>11.1.3. 爬虫项目</h3></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="startproject"></a>11.1.3.1. 创建项目</h4></div></div></div>
			
			<p>创建爬虫项目</p>
			<pre class="screen">
scrapy startproject project
			</pre>
			<p>在抓取之前，你需要新建一个Scrapy工程</p>
			<pre class="screen">
			
neo@MacBook-Pro ~/Documents % scrapy startproject crawler 
New Scrapy project 'crawler', using template directory '/usr/local/lib/python3.6/site-packages/scrapy/templates/project', created in:
    /Users/neo/Documents/crawler

You can start your first spider with:
    cd crawler
    scrapy genspider example example.com

neo@MacBook-Pro ~/Documents % cd crawler 
neo@MacBook-Pro ~/Documents/crawler % find .
.
./crawler
./crawler/__init__.py
./crawler/__pycache__
./crawler/items.py
./crawler/middlewares.py
./crawler/pipelines.py
./crawler/settings.py
./crawler/spiders
./crawler/spiders/__init__.py
./crawler/spiders/__pycache__
./scrapy.cfg

			
			</pre>
			<p>Scrapy 工程目录主要有以下文件组成：</p>
			<pre class="screen">
scrapy.cfg: 项目配置文件
middlewares.py : 项目 middlewares 文件
items.py: 项目items文件
pipelines.py: 项目管道文件
settings.py: 项目配置文件
spiders: 放置spider的目录
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="spider"></a>11.1.3.2. Spider</h4></div></div></div>
			
			<p>创建爬虫，名字是 netkiller, 爬行的地址是 netkiller.cn</p>
			<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy genspider netkiller netkiller.cn
Created spider 'netkiller' using template 'basic' in module:
  crawler.spiders.netkiller
neo@MacBook-Pro ~/Documents/crawler % find .
.
./crawler
./crawler/__init__.py
./crawler/__pycache__
./crawler/__pycache__/__init__.cpython-36.pyc
./crawler/__pycache__/settings.cpython-36.pyc
./crawler/items.py
./crawler/middlewares.py
./crawler/pipelines.py
./crawler/settings.py
./crawler/spiders
./crawler/spiders/__init__.py
./crawler/spiders/__pycache__
./crawler/spiders/__pycache__/__init__.cpython-36.pyc
./crawler/spiders/netkiller.py
./scrapy.cfg

			
			</pre>
			<p>打开 crawler/spiders/netkiller.py 文件，修改内容如下</p>
			<pre class="screen">
			
# -*- coding: utf-8 -*-
import scrapy


class NetkillerSpider(scrapy.Spider):
    name = 'netkiller'
    allowed_domains = ['netkiller.cn']
    start_urls = ['http://www.netkiller.cn/']

    def parse(self, response):
        for link in response.xpath('//div[@class="blockquote"]')[1].css('a.ulink'):
            # self.log('This url is %s' % link)
            yield {
                'name': link.css('a::text').extract(),
                'url': link.css('a.ulink::attr(href)').extract()
                }
            
        pass
			
			</pre>
			<p>运行爬虫</p>
			<pre class="screen">
			
neo@MacBook-Pro ~/Documents/crawler % scrapy crawl netkiller -o output.json
2017-09-08 11:42:30 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: crawler)
2017-09-08 11:42:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'crawler', 'FEED_FORMAT': 'json', 'FEED_URI': 'output.json', 'NEWSPIDER_MODULE': 'crawler.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['crawler.spiders']}
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-08 11:42:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-09-08 11:42:30 [scrapy.core.engine] INFO: Spider opened
2017-09-08 11:42:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-09-08 11:42:30 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-09-08 11:42:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn/robots.txt&gt; (referer: None)
2017-09-08 11:42:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn/&gt; (referer: None)
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Architect 手札'], 'url': ['../architect/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Developer 手札'], 'url': ['../developer/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller PHP 手札'], 'url': ['../php/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Python 手札'], 'url': ['../python/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Testing 手札'], 'url': ['../testing/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Java 手札'], 'url': ['../java/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Cryptography 手札'], 'url': ['../cryptography/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Linux 手札'], 'url': ['../linux/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller FreeBSD 手札'], 'url': ['../freebsd/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Shell 手札'], 'url': ['../shell/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Security 手札'], 'url': ['../security/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Web 手札'], 'url': ['../www/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Monitoring 手札'], 'url': ['../monitoring/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Storage 手札'], 'url': ['../storage/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Mail 手札'], 'url': ['../mail/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Docbook 手札'], 'url': ['../docbook/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Project 手札'], 'url': ['../project/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Database 手札'], 'url': ['../database/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller PostgreSQL 手札'], 'url': ['../postgresql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller MySQL 手札'], 'url': ['../mysql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller NoSQL 手札'], 'url': ['../nosql/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller LDAP 手札'], 'url': ['../ldap/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Network 手札'], 'url': ['../network/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Cisco IOS 手札'], 'url': ['../cisco/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller H3C 手札'], 'url': ['../h3c/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Multimedia 手札'], 'url': ['../multimedia/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Perl 手札'], 'url': ['../perl/index.html']}
2017-09-08 11:42:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://www.netkiller.cn/&gt;
{'name': ['Netkiller Amateur Radio 手札'], 'url': ['../radio/index.html']}
2017-09-08 11:42:31 [scrapy.core.engine] INFO: Closing spider (finished)
2017-09-08 11:42:31 [scrapy.extensions.feedexport] INFO: Stored json feed (28 items) in: output.json
2017-09-08 11:42:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 438,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 6075,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 9, 8, 3, 42, 31, 157395),
 'item_scraped_count': 28,
 'log_count/DEBUG': 31,
 'log_count/INFO': 8,
 'memusage/max': 49434624,
 'memusage/startup': 49434624,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 9, 8, 3, 42, 30, 931267)}
2017-09-08 11:42:31 [scrapy.core.engine] INFO: Spider closed (finished)
			
			</pre>
			<p>你会看到返回结果</p>
			<pre class="screen">
			
{'name': ['Netkiller Architect 手札'], 'url': ['../architect/index.html']}			
			
			</pre>
			<div class="section"><div class="titlepage"><div><div><h5 class="title"><a id="response.follow"></a>11.1.3.2.1. 翻页操作</h5></div></div></div>
				
				<p>下面我们演示爬虫翻页，例如我们需要便利这部电子书 https://netkiller.cn/linux/index.html，首先创建一个爬虫任务</p>
				<pre class="screen">
				
neo@MacBook-Pro ~/Documents/crawler % scrapy genspider book netkiller.cn
Created spider 'book' using template 'basic' in module:
  crawler.spiders.book
				
				</pre>
				<p>编辑爬虫任务</p>
				<pre class="screen">
				
# -*- coding: utf-8 -*-
import scrapy


class BookSpider(scrapy.Spider):
    name = 'book'
    allowed_domains = ['netkiller.cn']
    start_urls = ['https://netkiller.cn/linux/index.html']

    def parse(self, response):
        yield {'title': response.css('title::text').extract()};
        # 这里取出下一页连接地址
        next_page = response.xpath('//a[@accesskey="n"]/@href').extract_first() 
        self.log('Next page: %s' % next_page)
        # 如果页面不为空交给 response.follow 来爬取这个页面
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)    

        pass
				
				</pre>
				<pre class="screen">
				
				</pre>
				<pre class="screen">
				
				</pre>
			</div>

		</div>
	</div>
</div>
</div><div xmlns="" id="disqus_thread"></div><script xmlns="">

var disqus_config = function () {
this.page.url = "http://www.netkiller.cn";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'netkiller'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//netkiller.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript xmlns="">Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><br xmlns="" /><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="../3rdparty/index.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">10.18. zope.testbrowser </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 第 12 章 3rdparty toolkit</td></tr></table></div><script xmlns="">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11694057-1', 'auto');
  ga('send', 'pageview');

</script><script xmlns="" async="async">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script xmlns="" async="async">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><script xmlns="" type="text/javascript" src="/js/q.js" async="async"></script></body></html>